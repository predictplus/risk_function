{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "\n",
    "# Read the input data\n",
    "input_data = input(\"Enter the path to the dataset CSV file: \")\n",
    "\n",
    "#Add severity column index\n",
    "si = int(input(\"Enter the severity column index: \"))\n",
    "# 4 - stats19, 15 - crss, 1 - onisr\n",
    "\n",
    "# load data from CSV file using a CSV reader\n",
    "data = []\n",
    "severity = []\n",
    "with open(input_data, newline='') as csvfile:\n",
    "    reader = csv.reader(csvfile, delimiter=',')\n",
    "    header = next(reader) # skip the header row\n",
    "    for row in reader:\n",
    "        data.append(row[0:si-1] + row[si+1:-1])\n",
    "        severity.append(row[si])\n",
    "\n",
    "# split data into training and test sets\n",
    "data_train, data_test, severity_train, severity_test = train_test_split(\n",
    "    data, severity, test_size=0.25, stratify=severity, random_state=42)\n",
    "\n",
    "print(len(data_test), len(data_train))\n",
    "print(len(severity_test), len(severity_train))\n",
    "print(\"Severity distribution in train:\")\n",
    "print(str(pd.Series(severity_test).value_counts()))\n",
    "print(\"Severity distribution in train:\")\n",
    "print(str(pd.Series(severity_test).value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Ask the user to select from three different options\n",
    "print(\"Please select a dataset balancing option:\")\n",
    "print(\"Type 1 for weighted balancing\")\n",
    "print(\"Type 2 for oversampled balancing\")\n",
    "print(\"Type 3 for undersampled balancing\")\n",
    "\n",
    "# Get the user's input\n",
    "user_input = input(\"Enter your choice: \")\n",
    "\n",
    "# Handle the user's input using conditional statements\n",
    "\n",
    "if user_input == \"1\":\n",
    "    print(\"You selected weighted balancing\")\n",
    "    # Training our classifier with weighted severity classes\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    class_weights = [1 / pd.Series(severity_train).value_counts()[cls] for cls in unique_classes]\n",
    "    model = CatBoostClassifier(iterations=50, early_stopping_rounds=10, class_weights=class_weights, random_state=42, verbose=500)\n",
    "    model.fit(data_train, severity_train, eval_set=(data_test, severity_test))\n",
    "    model.save_model('weighed_catboost_model.cbm')\n",
    "\n",
    "elif user_input == \"2\":\n",
    "    print(\"You selected oversampled balancing\")\n",
    "    # Create a RandomOverSampler object\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = oversampler.fit_resample(data_train, severity_train)\n",
    "    model = CatBoostClassifier(iterations=50, early_stopping_rounds=10, class_weights=class_weights, random_state=42, verbose=500)\n",
    "    model.fit(data_train_resampled, severity_train_resampled, eval_set=(data_test, severity_test))\n",
    "    model.save_model('oversampled_catboost_model.cbm')\n",
    "\n",
    "elif user_input == \"3\":\n",
    "    print(\"You selected undersampled balancing\")\n",
    "    # Create a RandomUnderSampler object\n",
    "    undersampler = RandomUnderSampler(random_state=42)\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled_us, severity_train_resampled_us = undersampler.fit_resample(data_train, severity_train)    \n",
    "    model = CatBoostClassifier(iterations=50, early_stopping_rounds=10, class_weights=class_weights, random_state=42, verbose=500)\n",
    "    model.fit(data_train_resampled, severity_train_resampled, eval_set=(data_test, severity_test))\n",
    "    model.save_model('undersampled_catboost_model.cbm')    \n",
    "\n",
    "else:\n",
    "    print(\"Invalid choice. Please select a number between 1 and 3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assuming you have feature names in a list called 'feature_names'\n",
    "feature_names = header[0:si] + header[si+1:]\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = model.feature_importances_\n",
    "\n",
    "# Sort feature importances and their corresponding feature names in descending order\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "sorted_feature_importances = feature_importances[sorted_indices]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "# Create a bar chart with feature importances\n",
    "plt.figure(figsize=(22, 6))\n",
    "plt.bar(sorted_feature_names, sorted_feature_importances)\n",
    "plt.xticks(fontsize=8)  # Change the font size of the x-axis labels\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Printing features of the classifier with weighted severity classes\n",
    "accuracy = model.score(data_test, severity_test)\n",
    "print(f'Accuracy:{accuracy}')\n",
    "print(\"\")\n",
    "\n",
    "\"\"\"\n",
    "A confusion matrix is a performance evaluation tool used in CatBoost, which is a gradient boosting machine learning algorithm.\n",
    "The confusion matrix is a table that shows the number of true positives, true negatives, false positives, and false negatives for a given binary classification problem.\n",
    "In the context of CatBoost, the confusion matrix is generated by comparing the predicted labels of the model to the actual labels in the test dataset.\n",
    "The true positives (TP) represent the number of instances where the model predicted a positive label and the actual label was also positive.\n",
    "The true negatives (TN) represent the number of instances where the model predicted a negative label and the actual label was also negative.\n",
    "The false positives (FP) represent the number of instances where the model predicted a positive label, but the actual label was negative.\n",
    "The false negatives (FN) represent the number of instances where the model predicted a negative label, but the actual label was positive.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import confusion_matrix\n",
    "test_result = model.predict(data_test, prediction_type='Class')\n",
    "\n",
    "#calculating confusion matrix\n",
    "cm = confusion_matrix(severity_test, test_result)\n",
    "\n",
    "\"\"\"\n",
    "The confusion_matrix function output\n",
    "\n",
    "                Predicted\n",
    "             1     2     3\n",
    "    Actual  -----------------\n",
    "      1   | TP1 | FP1 | FP2 |\n",
    "      2   | FP3 | TP2 | FP4 |\n",
    "      3   | FP5 | FP6 | TP3 |\n",
    "\n",
    "TP represents the number of instances that are correctly classified,\n",
    "FP represents the number of instances that are incorrectly classified as class 2 when they actually belong to class 1,\n",
    "FP2 represents the number of instances that are incorrectly classified as class 3 when they actually belong to class 1,\n",
    "and so on.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "report = classification_report(severity_test, test_result)\n",
    "print(report)\n",
    "\n",
    "\"\"\"\n",
    "roc_auc_score is a performance metric used in CatBoost, which is a gradient boosting machine learning algorithm.\n",
    "The roc_auc_score is a measure of how well the algorithm is able to distinguish between two classes in a binary classification problem.\n",
    "The term \"ROC\" stands for Receiver Operating Characteristic, which is a curve that plots the true positive rate against the false positive rate for a given classification model.\n",
    "The area under this curve is referred to as the \"ROC AUC\" score, and is used to evaluate the performance of a binary classification model.\n",
    "In CatBoost, the roc_auc_score is used to evaluate the performance of the model on the test dataset.\n",
    "A higher roc_auc_score indicates that the model is better able to distinguish between the positive and negative classes in the test dataset, and therefore has a higher predictive accuracy.\n",
    "This metric is particularly useful when dealing with imbalanced datasets, where one class is much more prevalent than the other, as it provides a more robust measure of performance than simply looking at accuracy or precision.\n",
    "\"\"\"\n",
    "\n",
    "from sklearn.metrics import roc_auc_score\n",
    "proba_test_result = model.predict_proba(data_test)\n",
    "n_classes = len(model.classes_)\n",
    "\n",
    "import numpy as np\n",
    "acc_cons_low = np.sum(np.tril(cm)/np.sum(cm))\n",
    "acc_cons_up = np.sum(np.triu(cm)/np.sum(cm))\n",
    "print(f'confusion_matrix_lower_triangle: {acc_cons_low}')\n",
    "print(f'confusion_matrix_upper_triangle: {acc_cons_up}')\n",
    "\n",
    "#calcularing roc_auc_score\n",
    "auc_roc = roc_auc_score(severity_test, proba_test_result, multi_class='ovr', average='macro')\n",
    "print(f'AUC_ROC: {auc_roc}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot confusion matrix heatmap\n",
    "def plot_confusion_matrix(cm, class_labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Greens', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "plot_confusion_matrix(cm, model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot ROC AUC curves for each class\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert target labels to binary format using one-hot encoding\n",
    "y_test_bin = label_binarize(severity_test, classes=model.classes_)\n",
    "\n",
    "# Obtain predicted probabilities\n",
    "y_probs = model.predict_proba(data_test)\n",
    "\n",
    "# Compute ROC curve and ROC AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = len(model.classes_)\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC AUC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {model.classes_[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "facial-closing",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
