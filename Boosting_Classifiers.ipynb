{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "partial-liabilities",
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Read the input data\n",
    "input_data = input(\"Enter the path to the dataset CSV file: \")\n",
    "\n",
    "#Add severity column index\n",
    "si = int(input(\"Enter the severity column index: \"))\n",
    "# 7 - stats19, 14 - crss, 0 - BAAC\n",
    "\n",
    "# load data from CSV file using a CSV reader\n",
    "df = pd.read_csv(input_data)\n",
    "target_col_name = df.columns[si]\n",
    "data = df.drop([target_col_name], axis=1)\n",
    "severity = df[target_col_name]\n",
    "header = data.columns\n",
    "\n",
    "# split data into training and test sets\n",
    "data_train, data_test, severity_train, severity_test = train_test_split(\n",
    "    data, severity, test_size=0.25, stratify=severity, random_state=42)\n",
    "\n",
    "print(len(data_test), len(data_train))\n",
    "print(len(severity_test), len(severity_train))\n",
    "print(\"Severity distribution in train:\")\n",
    "print(str(pd.Series(severity_train).value_counts()))\n",
    "print(\"Severity distribution in test:\")\n",
    "print(str(pd.Series(severity_test).value_counts()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "engaged-coral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from catboost import CatBoostClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.utils.class_weight import compute_sample_weight\n",
    "from lightgbm import LGBMClassifier\n",
    "from lightgbm import early_stopping, log_evaluation\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Ask the user to select from three different options\n",
    "print(\"Please select a boosting model and dataset balancing option:\")\n",
    "print(\"Type 0 for Catboost with no balancing\")\n",
    "print(\"Type 1 for Catboost weighted balancing\")\n",
    "print(\"Type 2 for Catboost oversampled balancing\")\n",
    "print(\"Type 3 for Catboost undersampled balancing\")\n",
    "print(\"Type 4 for Catboost SMOTE balancing\")\n",
    "print(\"Type 5 for XGBoost with no balancing\")\n",
    "print(\"Type 6 for XGBoost weighted balancing\")\n",
    "print(\"Type 7 for XGBoost oversampled balancing\")\n",
    "print(\"Type 8 for XGBoost undersampled balancing\")\n",
    "print(\"Type 9 for XGBoost SMOTE balancing\")\n",
    "print(\"Type 10 for LGBM with no balancing\")\n",
    "print(\"Type 11 for LGBM weighted balancing\")\n",
    "print(\"Type 12 for LGBM oversampled balancing\")\n",
    "print(\"Type 13 for LGBM undersampled balancing\")\n",
    "\n",
    "# Get the user's input\n",
    "user_input = input(\"Enter your choice: \")\n",
    "\n",
    "# Handle the user's input using conditional statements\n",
    "\n",
    "if user_input == \"0\":\n",
    "    print(\"You have selected Catboost with no balancing\")\n",
    "    # Training our classifier with weighted severity classes\n",
    "    model = CatBoostClassifier(iterations=300, early_stopping_rounds=10, random_state=42)\n",
    "    model.fit(data_train, severity_train, eval_set=(data_test, severity_test), plot = True)\n",
    "    model_name = 'no_balance_catboost_model'\n",
    "    model.save_model(model_name + '.cbm')\n",
    "\n",
    "elif user_input == \"1\":\n",
    "    print(\"You have selected Catboost weighted balancing\")\n",
    "    # Training our classifier with weighted severity classes\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    class_weights = [1 / pd.Series(severity_train).value_counts()[cls] for cls in unique_classes]\n",
    "    model = CatBoostClassifier(iterations=300, early_stopping_rounds=10, class_weights=class_weights, random_state=42)\n",
    "    model.fit(data_train, severity_train, eval_set=(data_test, severity_test), plot = True)\n",
    "    model_name = 'weighed_catboost_model'\n",
    "    model.save_model(model_name + '.cbm')\n",
    "    \n",
    "elif user_input == \"2\":\n",
    "    print(\"You have selected Catboost oversampled balancing\")\n",
    "    # Create a RandomOverSampler object\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = oversampler.fit_resample(data_train, severity_train)\n",
    "    model = CatBoostClassifier(iterations=300, early_stopping_rounds=10, random_state=42)\n",
    "    model.fit(data_train_resampled, severity_train_resampled, eval_set=(data_test, severity_test), plot = True)\n",
    "    model_name = 'oversampled_catboost_model'\n",
    "    model.save_model(model_name + '.cbm')\n",
    "\n",
    "elif user_input == \"3\":\n",
    "    print(\"You have selected Catboost undersampled balancing\")\n",
    "    # Create a RandomUnderSampler object\n",
    "    undersampler = RandomUnderSampler(random_state=42)\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = undersampler.fit_resample(data_train, severity_train)    \n",
    "    model = CatBoostClassifier(iterations=300, early_stopping_rounds=10, random_state=42)\n",
    "    model.fit(data_train_resampled, severity_train_resampled, eval_set=(data_test, severity_test), plot = True)\n",
    "    model_name = 'undersampled_catboost_model'\n",
    "    model.save_model(model_name + '.cbm')\n",
    "    \n",
    "elif user_input == \"4\":\n",
    "    print(\"You have selected Catboost SMOTE balancing\")\n",
    "    # Create a SMOTE object\n",
    "    smote = SMOTE(random_state=42)\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = smote.fit_resample(data_train, severity_train)    \n",
    "    model = CatBoostClassifier(iterations=300, early_stopping_rounds=10, random_state=42)\n",
    "    model.fit(data_train_resampled, severity_train_resampled, eval_set=(data_test, severity_test), plot = True)\n",
    "    model_name = 'smote_catboost_model'\n",
    "    model.save_model(model_name + '.cbm')\n",
    "\n",
    "elif user_input == \"5\":\n",
    "    print(\"You have selected XGBoost with no balancing\")\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    if unique_classes[0] != 0:\n",
    "        class_map = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        severity_train = [class_map[cls] for cls in severity_train]\n",
    "        severity_test = [class_map[cls] for cls in severity_test]\n",
    "    else:\n",
    "        severity_train = severity_train\n",
    "        severity_test = severity_test\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    model = XGBClassifier(n_estimators=300, early_stopping_rounds=10, random_state=42)\n",
    "    model.fit(data_train, severity_train, eval_set=[(data_test, severity_test)])\n",
    "    model_name = 'no_balance_xgboost_model'\n",
    "    model.save_model(model_name + '.json')\n",
    "    \n",
    "elif user_input == \"6\":\n",
    "    print(\"You have selected XGBoost weighted balancing\")\n",
    "    # Training our classifier with weighted severity classes\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    if unique_classes[0] != 0:\n",
    "        class_map = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        severity_train = [class_map[cls] for cls in severity_train]\n",
    "        severity_test = [class_map[cls] for cls in severity_test]\n",
    "    else:\n",
    "        severity_train = severity_train\n",
    "        severity_test = severity_test\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    sample_weights = compute_sample_weight(class_weight='balanced', y=severity_train)\n",
    "    # Train the XGBoost classifier with the computed weights\n",
    "    model = XGBClassifier(n_estimators=300, early_stopping_rounds=10, random_state=42, objective='multi:softmax', eval_metric='mlogloss')\n",
    "    model.fit(data_train, severity_train, sample_weight=sample_weights, eval_set=[(data_test, severity_test)])\n",
    "    model_name = 'weighed_xgboost_model'\n",
    "    model.save_model(model_name + '.json')\n",
    "    \n",
    "elif user_input == \"7\":\n",
    "    print(\"You have selected XGBoost oversampled balancing\")\n",
    "    # Create a RandomOverSampler object\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    if unique_classes[0] != 0:\n",
    "        class_map = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        severity_train = [class_map[cls] for cls in severity_train]\n",
    "        severity_test = [class_map[cls] for cls in severity_test]\n",
    "    else:\n",
    "        severity_train = severity_train\n",
    "        severity_test = severity_test\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = oversampler.fit_resample(data_train, severity_train)\n",
    "    model = XGBClassifier(n_estimators=300, early_stopping_rounds=10, random_state=42)\n",
    "    model.fit(data_train_resampled, severity_train_resampled, eval_set=[(data_test, severity_test)])\n",
    "    model_name = 'oversampled_xgboost_model'\n",
    "    model.save_model(model_name + '.json')\n",
    "\n",
    "elif user_input == \"8\":\n",
    "    print(\"You have selected XGBoost undersampled balancing\")\n",
    "    # Create a RandomUnderSampler object\n",
    "    undersampler = RandomUnderSampler(random_state=42)\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    if unique_classes[0] != 0:\n",
    "        class_map = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        severity_train = [class_map[cls] for cls in severity_train]\n",
    "        severity_test = [class_map[cls] for cls in severity_test]\n",
    "    else:\n",
    "        severity_train = severity_train\n",
    "        severity_test = severity_test\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = undersampler.fit_resample(data_train, severity_train)    \n",
    "    model = XGBClassifier(n_estimators=300, early_stopping_rounds=10, random_state=42)\n",
    "    model.fit(data_train_resampled, severity_train_resampled, eval_set=[(data_test, severity_test)])\n",
    "    model_name = 'undersampled_xgboost_model'\n",
    "    model.save_model(model_name + '.json')\n",
    "    \n",
    "elif user_input == \"9\":\n",
    "    print(\"You have selected XGBoost SMOTE balancing\")\n",
    "    # Create a SMOTE object\n",
    "    smote = SMOTE(random_state=42)\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    if unique_classes[0] != 0:\n",
    "        class_map = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        severity_train = [class_map[cls] for cls in severity_train]\n",
    "        severity_test = [class_map[cls] for cls in severity_test]\n",
    "    else:\n",
    "        severity_train = severity_train\n",
    "        severity_test = severity_test\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = smote.fit_resample(data_train, severity_train)    \n",
    "    model = XGBClassifier(n_estimators=300, early_stopping_rounds=10, random_state=42)\n",
    "    model.fit(data_train_resampled, severity_train_resampled, eval_set=[(data_test, severity_test)])\n",
    "    model_name = 'smote_xgboost_model'\n",
    "    model.save_model(model_name + '.json')\n",
    "    \n",
    "elif user_input == \"10\":\n",
    "    print(\"You have selected LGBM with no balancing\")\n",
    "    model = LGBMClassifier(objective='multiclass',\n",
    "                       num_class=len(unique_classes),\n",
    "                       metric='multi_logloss',\n",
    "                       boosting_type='gbdt',\n",
    "                       learning_rate=0.1,\n",
    "                       feature_fraction=0.9,\n",
    "                       bagging_fraction=0.8,\n",
    "                       bagging_freq=5,\n",
    "                       n_estimators=1000,\n",
    "                       random_state=42)\n",
    "    model.fit(X=data_train, \n",
    "          y=severity_train, \n",
    "          eval_set=[(data_test, severity_test)],\n",
    "          callbacks=[early_stopping(10), log_evaluation(period=100)])  # Print every 100 iterations\n",
    "    model_name = 'no_balance_lgbm_model'\n",
    "    model.booster_.save_model(model_name + '.txt')\n",
    "\n",
    "elif user_input == \"11\":\n",
    "    print(\"You have selected LGBM weighted balancing\")\n",
    "    # Assuming you already have data_train, data_test, severity_train, and severity_test\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    class_weights = [1 / pd.Series(severity_train).value_counts()[cls] for cls in unique_classes]\n",
    "    class_weight_dict = dict(zip(unique_classes, class_weights))\n",
    "    # Create weights for training data\n",
    "    weights = severity_train.map(class_weight_dict)\n",
    "    model = LGBMClassifier(objective='multiclass',\n",
    "                       num_class=len(unique_classes),\n",
    "                       metric='multi_logloss',\n",
    "                       boosting_type='gbdt',\n",
    "                       learning_rate=0.1,\n",
    "                       feature_fraction=0.9,\n",
    "                       bagging_fraction=0.8,\n",
    "                       bagging_freq=5,\n",
    "                       n_estimators=1000,\n",
    "                       random_state=42)\n",
    "    model.fit(X=data_train, \n",
    "          y=severity_train,\n",
    "          sample_weight=weights,\n",
    "          eval_set=[(data_test, severity_test)],\n",
    "          callbacks=[early_stopping(10), log_evaluation(period=100)])  # Print every 100 iterations\n",
    "    model_name = 'weighed_lgbm_model'\n",
    "    model.booster_.save_model(model_name + '.txt')\n",
    "    \n",
    "elif user_input == \"12\":\n",
    "    print(\"You have selected LGBM oversampled balancing\")\n",
    "    # Create a RandomOverSampler object\n",
    "    oversampler = RandomOverSampler(random_state=42)\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    if unique_classes[0] != 0:\n",
    "        class_map = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        severity_train = [class_map[cls] for cls in severity_train]\n",
    "        severity_test = [class_map[cls] for cls in severity_test]\n",
    "    else:\n",
    "        severity_train = severity_train\n",
    "        severity_test = severity_test\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = oversampler.fit_resample(data_train, severity_train)\n",
    "    # Create weights for training data\n",
    "    model = LGBMClassifier(objective='multiclass',\n",
    "                       num_class=len(unique_classes),\n",
    "                       metric='multi_logloss',\n",
    "                       boosting_type='gbdt',\n",
    "                       learning_rate=0.1,\n",
    "                       feature_fraction=0.9,\n",
    "                       bagging_fraction=0.8,\n",
    "                       bagging_freq=5,\n",
    "                       n_estimators=1000,\n",
    "                       random_state=42)\n",
    "    model.fit(X=data_train_resampled, \n",
    "          y=severity_train_resampled,\n",
    "          eval_set=[(data_test, severity_test)],\n",
    "          callbacks=[early_stopping(10), log_evaluation(period=100)])  # Print every 100 iterations\n",
    "    model_name = 'oversampled_lgbm_model'\n",
    "    model.booster_.save_model(model_name + '.txt')    \n",
    "    \n",
    "elif user_input == \"13\":\n",
    "    print(\"You have selected LGBM undersampled balancing\")\n",
    "    # Create a RandomUnderSampler object\n",
    "    undersampler = RandomUnderSampler(random_state=42)\n",
    "    unique_classes = np.unique(severity_train)\n",
    "    if unique_classes[0] != 0:\n",
    "        class_map = {cls: idx for idx, cls in enumerate(unique_classes)}\n",
    "        severity_train = [class_map[cls] for cls in severity_train]\n",
    "        severity_test = [class_map[cls] for cls in severity_test]\n",
    "    else:\n",
    "        severity_train = severity_train\n",
    "        severity_test = severity_test\n",
    "    # Resample the training data and labels\n",
    "    data_train_resampled, severity_train_resampled = oversampler.fit_resample(data_train, severity_train)\n",
    "    # Create weights for training data\n",
    "    model = LGBMClassifier(objective='multiclass',\n",
    "                       num_class=len(unique_classes),\n",
    "                       metric='multi_logloss',\n",
    "                       boosting_type='gbdt',\n",
    "                       learning_rate=0.1,\n",
    "                       feature_fraction=0.9,\n",
    "                       bagging_fraction=0.8,\n",
    "                       bagging_freq=5,\n",
    "                       n_estimators=1000,\n",
    "                       random_state=42)\n",
    "    model.fit(X=data_train_resampled, \n",
    "          y=severity_train_resampled,\n",
    "          eval_set=[(data_test, severity_test)],\n",
    "          callbacks=[early_stopping(10), log_evaluation(period=100)])  # Print every 100 iterations\n",
    "    model_name = 'undersampled_lgbm_model'\n",
    "    model.booster_.save_model(model_name + '.txt')    \n",
    "\n",
    "else:\n",
    "    print(\"Invalid choice. Please select a number between 1 and 3.\")\n",
    "    \n",
    "print(\"Training and Fitting is Done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-powell",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "# Calculate correlation matrix\n",
    "correlation_matrix = data.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap=\"YlGnBu\", annot_kws={\"fontsize\": 6})\n",
    "plt.title(\"Correlation Matrix\")\n",
    "plt.show()\n",
    "\n",
    "# Assuming you have feature names in a list called 'feature_names'\n",
    "feature_names = header\n",
    "\n",
    "# Get feature importances from the trained model\n",
    "feature_importances = model.feature_importances_\n",
    "feature_importances = np.round((feature_importances/np.sum(feature_importances)),3)\n",
    "print(feature_importances)\n",
    "\n",
    "# Sort feature importances and their corresponding feature names in descending order\n",
    "sorted_indices = feature_importances.argsort()[::-1]\n",
    "sorted_feature_importances = feature_importances[sorted_indices]\n",
    "sorted_feature_names = [feature_names[i] for i in sorted_indices]\n",
    "\n",
    "df_feat_imp = pd.DataFrame(sorted_feature_importances, index = sorted_feature_names)\n",
    "df_feat_imp.to_csv(\"fi_\" + model_name + \".csv\", index = True, header = False)\n",
    "\n",
    "# Create a bar chart with feature importances\n",
    "plt.figure(figsize=(22, 6))\n",
    "plt.bar(sorted_feature_names, sorted_feature_importances)\n",
    "plt.xticks(fontsize=7)  # Change the font size of the x-axis labels\n",
    "plt.xlabel('Features', fontsize=12)\n",
    "plt.ylabel('Importance', fontsize=12)\n",
    "plt.title('Feature Importances')\n",
    "\n",
    "# Display the chart\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-asset",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score, roc_auc_score, confusion_matrix\n",
    "import numpy as np\n",
    "\n",
    "#Printing features of the classifier with weighted severity classes\n",
    "y_pred = model.predict(data_test)\n",
    "y_proba = model.predict_proba(data_test) # For ROC AUC\n",
    "accuracy = accuracy_score(severity_test, y_pred)\n",
    "\n",
    "\"\"\"\n",
    "A confusion matrix is a performance evaluation tool used in CatBoost, which is a gradient boosting machine learning algorithm.\n",
    "The confusion matrix is a table that shows the number of true positives, true negatives, false positives, and false negatives for a given binary classification problem.\n",
    "In the context of CatBoost, the confusion matrix is generated by comparing the predicted labels of the model to the actual labels in the test dataset.\n",
    "The true positives (TP) represent the number of instances where the model predicted a positive label and the actual label was also positive.\n",
    "The true negatives (TN) represent the number of instances where the model predicted a negative label and the actual label was also negative.\n",
    "The false positives (FP) represent the number of instances where the model predicted a positive label, but the actual label was negative.\n",
    "The false negatives (FN) represent the number of instances where the model predicted a negative label, but the actual label was positive.\n",
    "\"\"\"\n",
    "\n",
    "test_result = model.predict(data_test)\n",
    "#calculating confusion matrix\n",
    "cm = confusion_matrix(severity_test, y_pred)\n",
    "proba_test_result = model.predict_proba(data_test)\n",
    "n_classes = len(model.classes_)\n",
    "\n",
    "acc_cons_dwn = np.sum(np.tril(cm)/np.sum(cm))\n",
    "acc_cons_up = np.sum(np.triu(cm)/np.sum(cm))\n",
    "print(f'confusion_matrix_lower_triangle: {acc_cons_dwn}')\n",
    "print(f'confusion_matrix_upper_triangle: {acc_cons_up}')\n",
    "\n",
    "\"\"\"\n",
    "The confusion_matrix function output\n",
    "\n",
    "                Predicted\n",
    "             1     2     3\n",
    "    Actual  -----------------\n",
    "      1   | TP1 | FP1 | FP2 |\n",
    "      2   | FP3 | TP2 | FP4 |\n",
    "      3   | FP5 | FP6 | TP3 |\n",
    "\n",
    "TP represents the number of instances that are correctly classified,\n",
    "FP represents the number of instances that are incorrectly classified as class 2 when they actually belong to class 1,\n",
    "FP2 represents the number of instances that are incorrectly classified as class 3 when they actually belong to class 1,\n",
    "and so on.\n",
    "\"\"\"\n",
    "\n",
    "#calcularing roc_auc_score\n",
    "# Multi-class case with OneVsRest strategy\n",
    "roc_auc = roc_auc_score(severity_test, y_proba, multi_class='ovr', average='macro')\n",
    "print(f'AUC_ROC: {roc_auc}')\n",
    "\n",
    "\"\"\"\n",
    "roc_auc_score is a performance metric used in CatBoost, which is a gradient boosting machine learning algorithm.\n",
    "The roc_auc_score is a measure of how well the algorithm is able to distinguish between two classes in a binary classification problem.\n",
    "The term \"ROC\" stands for Receiver Operating Characteristic, which is a curve that plots the true positive rate against the false positive rate for a given classification model.\n",
    "The area under this curve is referred to as the \"ROC AUC\" score, and is used to evaluate the performance of a binary classification model.\n",
    "In CatBoost, the roc_auc_score is used to evaluate the performance of the model on the test dataset.\n",
    "A higher roc_auc_score indicates that the model is better able to distinguish between the positive and negative classes in the test dataset, and therefore has a higher predictive accuracy.\n",
    "This metric is particularly useful when dealing with imbalanced datasets, where one class is much more prevalent than the other, as it provides a more robust measure of performance than simply looking at accuracy or precision.\n",
    "\"\"\"\n",
    "\n",
    "report = classification_report(severity_test, y_pred, output_dict=True)\n",
    "report['accuracy'] = {\" \": accuracy}\n",
    "report['ROC AUC'] = {\" \": roc_auc}\n",
    "report['acc_cons_up'] = {\" \": acc_cons_up}\n",
    "report['acc_cons_dwn'] = {\" \": acc_cons_dwn}\n",
    "\n",
    "print(report)\n",
    "\n",
    "# Flatten the reports\n",
    "flat_report = {}\n",
    "for class_label, class_report in report.items():\n",
    "    if isinstance(class_report, dict):\n",
    "        for metric, score in class_report.items():\n",
    "            if metric != 'support':\n",
    "                flat_report[(model, class_label, metric)] = score\n",
    "\n",
    "# Convert the flat reports to a DataFrame\n",
    "df_report = pd.DataFrame(list(flat_report.items()), columns=['Model_Class_Metric', 'Score'])\n",
    "\n",
    "# Split the Model_Class_Metric tuple into separate columns\n",
    "df_report[['Model', 'Class', 'Metric']] = pd.DataFrame(df_report['Model_Class_Metric'].tolist(), index=df_report.index)\n",
    "\n",
    "# Drop the Model_Class_Metric column\n",
    "df_report = df_report.drop(columns='Model_Class_Metric')\n",
    "\n",
    "# Reorder the columns\n",
    "df_report = df_report[['Model', 'Class', 'Metric', 'Score']]\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df_report.to_csv('cr_' + model_name + '.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rocky-origin",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import numpy as np\n",
    "\n",
    "# Function to plot confusion matrix heatmap\n",
    "def plot_confusion_matrix(cm, class_labels):\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Reds', xticklabels=class_labels, yticklabels=class_labels)\n",
    "    plt.xlabel('Predicted Label')\n",
    "    plt.ylabel('True Label')\n",
    "    plt.title('Confusion Matrix')\n",
    "\n",
    "# Plot confusion matrix heatmap\n",
    "plot_confusion_matrix(cm, model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "polar-medicine",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to plot ROC AUC curves for each class\n",
    "import numpy as np\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import label_binarize\n",
    "\n",
    "# Convert target labels to binary format using one-hot encoding\n",
    "y_test_bin = label_binarize(severity_test, classes=model.classes_)\n",
    "\n",
    "# Obtain predicted probabilities\n",
    "y_probs = model.predict_proba(data_test)\n",
    "\n",
    "# Compute ROC curve and ROC AUC for each class\n",
    "fpr = dict()\n",
    "tpr = dict()\n",
    "roc_auc = dict()\n",
    "n_classes = len(model.classes_)\n",
    "for i in range(n_classes):\n",
    "    fpr[i], tpr[i], _ = roc_curve(y_test_bin[:, i], y_probs[:, i])\n",
    "    roc_auc[i] = auc(fpr[i], tpr[i])\n",
    "\n",
    "# Plot ROC AUC curves\n",
    "plt.figure(figsize=(8, 6))\n",
    "for i in range(n_classes):\n",
    "    plt.plot(fpr[i], tpr[i], label=f'Class {model.classes_[i]} (AUC = {roc_auc[i]:.2f})')\n",
    "\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC AUC')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "diverse-count",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
